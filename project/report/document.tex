\documentclass[12pt]{article}

\usepackage{sbc-template}

\usepackage{graphicx,url}
\usepackage{csvsimple}
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{amsmath}
\usepackage{multirow}
\usepackage[brazil]{babel}    
\usepackage[utf8]{inputenc}
 
\sloppy

\title{Simulação de \textit{tracking} da dinâmica dos elétrons em um acelerador
síncrotron}

\author{Gustavo Ciotto Pinton\inst{1} - RA117136 }


\address{Instituto de Computação -- Universidade Estadual de Campinas
(UNICAMP)\\
  Av. Albert Einstein, 1251, Cidade Universitária, Campinas/SP \\
  Brasil, CEP 13083-852, Fone: [19] 3521-5838
  \email{gustavociotto@gmail.com}
}

\begin{document} 

\maketitle

\begin{abstract}
This report presents and describes the main aspects of the development of a
parallel solution for a sequential algorithm, whose purpose is simulating
the dynamics (modeled by a 6-element vector \(X\)) of electrons that
run through a synchrotron accelerator. In general, in this algorithm, the whole
extension of the accelerator is divided into several elements capable of
changing the state of each electron in a certain way, and it is sought to
determine which initial conditions of \(X\) will meet some conditions after a
certain number of revolutions. All the parallelization was based on GPUs and the
CUDA library, obtaining gains of performance of the order of 3000\% in relation
to the serial execution.
\end{abstract}
     
\begin{resumo} 
Este relatório apresenta e descreve os principais aspectos do
desenvolvimento de uma solução paralela para um algoritmo sequencial de
simulação da dinâmica do movimento dos elétrons (modelado por um vetor \(X\) de
6 posições) que percorrem um acelerador síncrotron. De maneira geral, neste
algoritmo, divide-se toda a extensão do acelerador em diversos elementos capazes de alterar o estado de cada életron de
uma determinada maneira, e busca-se quais condições iniciais de \(X\) atenderão
algumas condições após um determinado número de voltas. Toda a paralelização foi
baseada em GPUs e na biblioteca CUDA, obtendo-se ganhos de performance da ordem
de 3000\% em relação à execução serial.
\end{resumo}


\section{Introdução}

Aceleradores de partículas síncrotron são constituídos, ao longo de todas suas
circunferências, de atuadores, tais como dipolos, quadrupólos e sextupólos,
responsáveis por modificar a direção dos elétrons através da imposição de campos
magnéticos. Pode-se modelar o estado de um elétron na entrada do acelerador por
um vetor no espaço de fase constituído por 6 elementos \( X = (x_1, x_2, \ldots,
x_6) \). A cada passagem por um determinado atuador, esse vetor é modificado por
um mapa \(F_n(X)\) que depende, evidentemente, do tipo de atuador e de seus
parâmetros. A simulação de \textit{tracking} da dinâmica de movimento dos
elétrons consiste, portanto, em determinar quais vetores iniciais \(X_i\) neste
espaço de fase correspondem à órbitas estáveis após o percurso de \(N\) voltas
pelo acelerador. Um vetor \(X_i\) pode ser considerado uma
órbita estável somente se, ao fim de \(N\) voltas completas, as posições \(x_1,
\ldots, x_6 \) são inferiores às constantes \(C_1, \ldots, C_6\). A explicação
dos significados físicos de cada uma dessas posições não faz parte do escopo
deste artigo.

Durante os experimentos, o acelerador foi modelado por um número
constante \(M = 10000\) de atuadores e a cardinalidade do conjunto de vetores
iniciais testados foi igual a \(I = 10000\). Além disso, variou-se o número de
voltas \(N\) entre 10 e 10000, de modo a avaliar o ganho de performance em
função do número total de iterações desenvolvidas.

Aproveitando-se a alta densidade de \textit{cores} de arquitetura SIMD
encontrada nas GPUs atuais, a paralelização desta simulação consistiu em
distribuir o cálculo de cada condição inicial \(X_i\) em uma \textit{thread}
distinta executada pela GPU, de modo a maximizar a quantidade de
\textit{threads} que rodam simultaneamente. A implementação foi a baseada no
\textit{toolkit CUDA}, desenvolvido para as placas de vídeo da \textit{NVIDIA}.
Como todos os vetores \(X_i\) passarão pelos mesmos atuadores na mesma ordem e
não farão acessos a posições não sequenciais de memória, a paralelização deste
algoritmo não foi afetado pelos principais fatores que degradam a performance
neste tipo de arquitetura, sendo eles, respectivamente, \textit{branching
divergence} e acessos \textit{non-coalesced} à memória.

As próximas seções são dedicadas aos processos de implementação e aos resultados
obtidos.

\section {Análise da execução sequencial}

Com o intuito de identificar qual trecho do programa possui o maior potencial de
paralelização, a ferramenta \texttt{gprof} foi utilizada. Esse aplicativo é
capaz de calcular o tempo de execução gasto para cada função, permitindo, assim,
ao desenvolvedor uma visão global do desempenho do seu programa. Em
essência, duas análises sõa oferecidas por ele, sendo elas, a \textit {flat
profile} e a \textit{call graph}. A primeira mostra quanto tempo o programa
gastou em cada função e quantas vezes tal função foi chamada. A segunda, por sua
vez, permite, para cada função, visualizar quais outras funções ela chamou
e por quais outras funções ela foi chamada. Essa análise pode ser útil para a
determinação de trechos em que chamadas que tomam muito tempo podem ser
economizadas.

A tabela \ref{tab:flat} abaixo corresponde à fração mais relevante da análise
\textit{flat profile}, obtida através da execução da versão sequencial da
simulação com \(N = 10\). 

\begin{table}[h]
    \centering
    \small
	\caption{\label{tab:flat} Análise \textit{flat profile} da execução sequencial.}
	\begin{tabular}{| p{0.09\textwidth} | p{0.1\textwidth} | p{0.1\textwidth} |
	p{0.125\textwidth} | c | }
		\hline
		\textbf{\% time} & \textbf{cumul. seconds} & \textbf{self seconds} &
		\textbf{calls} & \textbf{name} \\ \hline 
		28.87 & 2.27 & 2.27  & 100000  & DynamicSearch::\textbf{performOneTurn(double (\&))} \\\hline 
		15.77 & 3.51 & 1.24  & 1200000020 & std::vector\(<\)RingElement\(>\)::\textbf{size()} const \\\hline 
		15.58 & 4.74 & 1.23  & 400000000  & Drift::\textbf{pass(double (\&))} \\\hline
		14.12 & 5.85 & 1.11  & 400000000  & Quadrupole::\textbf{pass(double (\&)} \\\hline
		11.64 & 6.77 & 0.92  & 200000000  & Sextupole::\textbf{pass(double (\&)} \\\hline
		11.45 & 7.67 & 0.90  & 100000000  & std::vector::\textbf{operator[]}(unsigned
		long) \\\hline \ldots & & & & \\\hline
	\end{tabular}
\end{table}

Observa-se, por meio dela, que 97.43\% de todo o tempo
gasto pelo programa está concentrado em apenas 6 funções. A análise do corpo da
função \texttt{dynamical\_aperture\_search()} (contido no arquivo
\texttt{DynamicSearch.cpp}) explica perfeitamente a afirmação anterior, à medida
que explicita três \textit{loops} encadeados. Os dois primeiros são responsáveis
por iterar sobre o conjunto de condições iniciais (de cardinalidade \(I\)),
enquanto que o segundo, por submeter cada uma das condições iniciais a \(N\)
voltas pelos atuadores. A cada iteração do laço mais interior, realiza-se uma
chamada à função \texttt{performOneTurn(double *)}, que, por sua vez, chama o
método \texttt{pass} de cada um dos \(M\) atuadores.
Desta forma, em linhas gerais, \(I * N * M\) iterações serão executadas apenas
neste trecho de código. É importante ressaltar que os dois \textit{loops}
responsáveis pela iteração sobre as condições iniciais podem ser classificados
como \textit{doall loops} e, portanto, podem ser paralelizados. Isso porque o
processo de iteração de uma condição inicial \(X_{i1}\) é
totalmente independente do processo associado à condição
\(X_{i2}\). Em oposição, o laço mais interior e aquele presente na função
\texttt{performOneTurn} são classificados como \textit{doacross loops}, visto
que as iterações executadas por eles dependem obrigatoriamente de resultados
gerados por iterações anteriores. Essa afirmação é facilmente verificada em
ambos os casos: para que a volta de índice \(i\) seja executada, ela
necessita dos resultados que as de índices \(i - 1, i - 2, \ldots, 0\)
produziram, e para que o estado do életron seja calculado no atuador \(M_i\), é
necessário que os estados associados aos atuadores \(M_{i-1}, M_{i-2},\ldots,
M_{0}\) sejam conhecidos.

% \begin{table}[h]
%     \centering
%     \small
% 	\caption{\label{tab:call} Análise \textit{call graph} da execução sequencial.}
% 	\begin{tabular}{| p{0.09\textwidth} | p{0.1\textwidth} | p{0.10\textwidth} | l | }
% 		\hline
% 		\textbf{\% time} & \textbf{self seconds} & \textbf{children}  & \textbf{name} \\ \hline 
% 		 & 2.27 & 5.39 & \textbf{DefaultDynamicSearch::dynamical\_aperture\_search()} \\\hline
% 		 \textbf{97.3} & 2.27 & 5.39 & DynamicSearch::\textbf{performOneTurn(double (\&))} \\\hline
% 		 & 1.24 & 0 & std::vector::operator[](unsigned long) \\\hline
% 		 & 1.24 & 0 & std::vector::operator[](unsigned long) \\\hline
% 		 & 1.23 & 0 & Drift::\textbf{pass(double (\&))} \\\hline
% 		 & 1.11 & 0 & Quadrupole::\textbf{pass(double (\&)} \\\hline
% 		 & 0.92 & 0 & Sextupole::\textbf{pass(double (\&)} \\\hline
% 		 \ldots & & & \\\hline
% 	\end{tabular}
% \end{table}

\section {Paralelização}

Esta seção é dedicada ao detalhamento do processo de implementação de um
algoritmo paralelo baseado em CUDA e das principais dificuldades encontradas e
como elas foram resolvidas.

\subsection{Implementação}

A fim de paralelizar a execução dos \textit{doall loops} detectados na seção
anterior, distribui-se a computação de cada condição inicial em uma
\textit{thread} distinta. Como discutido nesta mesma seção, isso pode ser
realizado dado que tais cálculos são completamente independentes entre si.

CUDA distribui as \textit{threads} em blocos unidimensionais, bidimensionais ou
tridimensionais de até 1024 \textit{threads}. Essa limitação existe devido ao
fato de que todas as \textit{threads} de um mesmo bloco residem no mesmo
\textit{core} e, desta forma, compartilham memória e tempo de processamento
\cite{cuda}. A multidimensionalidade dos blocos garante que as \textit{threads}
sejam identificadas mais facilmente de acordo com a sua aplicação. No nosso
caso, por exemplo, como as condições inicias são determinadas a partir de uma
tupla \((i,j)\), utilizamos blocos de duas dimensões. Adicionalmente, tendo em
vista que a quantidade máxima de \textit{threads} por bloco é 1024, adota-se,
neste artigo, blocos de \(32\times32\). Assim como as \textit{threads}, blocos
também são distribuídos em \textit{grids} de uma, duas ou três dimensões. Tendo
em vista que a motivação para este tipo de divisão é a mesma que a explicada
anteriormente, utilizaremos também \textit{grids} bidimensionais com dimensões
\(\frac{I_x}{32}\times\frac{I_y}{32}\), em que \(I_x\) e \(I_y\) representam as
dimensões do conjunto de condições iniciais.

\textit{Threads} são executadas simultaneamente em blocos de 32, denominados
\textit{warps}. Tais blocos rodam em uma arquitetura SIMD, isto é, uma única
instrução é capaz de realizar a mesma operação para todas as suas
\textit{threads}. \textit{Warps} são executadas paralelamente por
\textit{streaming multiprocessors}, ou simplesmente \textit{SMs}, cuja
capacidade depende da \textit{compute capability} da GPU. A escolha do número de
\textit{threads} por bloco também deve ser influenciada por essa capacidade, já
que deseja-se maximizar o uso de um \textit{SM}. Para a GPU \textit{Geforce
GTX1080}, que foi utilizada nos testes, o número máximo de \textit{warps} por
\textit{SM} é 64, totalizando, desta forma, 2048 \textit{threads} que podem ser
executadas simultaneamente. A escolha de blocos de 1024
\textit{threads} maxima, portanto, o uso de um \textit{SM}, à medida que 2
blocos podem ser rodados paralelamente, ocupando, assim, toda a capacidade do
respectivo \textit{SM}.

Com a divisão de \textit{threads} em blocos e \textit{warps} já definida, a
próxima etapa é implementar o \textit{kernel} que será executado em cada uma
delas e que calculará, a partir de uma condição inicial \(X_i\), o respectivo
vetor \(X_f\) após \(N\) voltas por \(M\)
atuadores. Tal função recebe como parâmetro, além de \(N\) e \(M\), o vetor
\(A\) de \texttt{struct} contendo os atributos que descrevem cada um dos \(M\)
atuadores. É importante ressaltar que, inicialmente, a ideia era transmitir um
vetor de objetos que herdassem da classe abstrata \texttt{DynamicSearch} e que
possuíssem as suas próprias implementações dos métodos virtuais. Porém, conforme
discutido na subseção \textbf{Dificuldades}, esse tipo de operação não é
suportado por \textit{CUDA}. Adicionalmente, o \textit{kernel} recebe também
como paramêtro o vetor $R$, cujo propósito é armazenar os resultados calculados
por todas as \textit{threads}, de maneira que eles possam ser copiados para a
memória da CPU posteriormente. Enfim, como a condição inicial depende dos
índices \((i,j)\) da \textit{thread} em que ela será utilizada, ela pode ser
calculada internamente e não necessita, portanto, ser transmitida ao
\textit{kernel} através de parâmetros. O pseudo-código \ref{alg:cuda}, a seguir,
exemplica as operações desenvolvidas por cada \textit{thread} na GPU.

\begin{algorithm}
\caption{\label{alg:cuda} \textit{Pseudo-código} do \textit{kernel} que é
executado pela GPU} \begin{algorithmic}[1]
  \Function{dynamicSearchKernel}{$A,M,N,R$}
  	\State ${i} \gets \text{blockDim.y} * \text{blockIdx.y} + \text{threadIdx.y} $ \Comment{Índice y da \textit{thread}} 
  	\State ${j} \gets \text{blockDim.x} * \text{blockIdx.x} + \text{threadIdx.x} $ \Comment{Índice x da \textit{thread}}  
    \State ${X_i} \gets f(i,j) $ \Comment{Calcula condição inicial \(X_i\) em função de \(i\) e \(j\)}
    \For{$n \gets 0$ to ${N}$} \Comment{Itera sobre o número de voltas}
        \For{$m \gets 0$ to ${M}$} \Comment{Itera sobre o número de atuadores}
        	\State $X_i \gets g_{A[m]} (X_i)$ \Comment{Altera \(X_i\) de acordo com atuador \(A[m]\)}
    	\EndFor
    \EndFor
    \State ${R[i,j]} \gets X_i $ \Comment{Armazena resultado calculado}
   \EndFunction

\end{algorithmic}
\end{algorithm}

Vale lembrar que \(A\) e \(R\) devem ser alocados na memória do dispositivo
através da chamada da função \texttt{cudaMalloc} e seus respectivos conteúdos
devem ser copiados de/para a GPU através de \texttt{cudaMemCpy}. Ao fim, as
memórias alocados são liberadas novamente por meio da função \texttt{memFree}.

\subsection{Dificuldades}

\section{Resultados}



\section{Conclusões}

\bibliographystyle{sbc}
\bibliography{sbc-template}

\end{document}
